For wetware, metacognition is typically defined as "thinking about thinking" or often a catch-all for any "higher-level" cognition. 

(In some more specific domains, it's an introspective process, focused on thinking about exclusively *your own* thinking or a suite of personal learning strategies...all valid within their purview, but too constrained for our purposes.)

In large language models, the synthetic corollary of cognition is inference. So we can reasonably define a metacognitive process in an LLM as any that runs inference on the output of prior inference. That is, inference itself is used as context--*inference about inference*. It might be instantly injected into the next prompt, stored for later use, or leveraged by another model. Experiments here will be critical to overcome [[The machine learning industry is too focused on general task performance|the machine learning community's fixation on task completion]].